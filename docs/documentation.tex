\documentclass[12pt]{article}
\usepackage{pgf}
\usepackage{minted}
\title{Analysis of Various Sorting Algorithms}
\author{Deebakkarthi C R (CB.EN.U4CSE20613) \\ \and
Pravin Sabari Bala (CB.EN.U4CSE20648)}

\begin{document}
\begin{titlepage}
\maketitle
\pagebreak
\end{titlepage}
\tableofcontents
\pagebreak
\section{Comparisons of various sorting algorithms}
\subsection{Testing Data Generation}
The input data for this was using the \texttt{random.random()} function in 
python. The length of the input ranged from 100 to 10000.
\subsection{Taking Measurements}
\textit{Swaps, Comparisons and basic operations} were calculated by updating
a global variable. \textit{Time taken} was measured using
\texttt{perf\_counter\_ns()} and \textit{memory} was measured using
\texttt{tracemalloc()}
\subsection{Plot Generation}
The measurements were store in a \textit{csv} file which was then used by
\texttt{matplotlib} to produce the plots. The y-scale of certain plots which
were too large are presented in a \textit{log} scale.
\subsection{Theoretical time complexities}
\begin{table}[!h]
\begin{tabular}{|l|l|l|l|l|}
\hline
Algorithm & Best            & Average         & Worst      & Worst(Space)        \\ \hline
Quick     & $\Omega(nlogn)$ & $\Theta(nlogn)$ & $O(n^2)$   & $O(n) / O(1)$              \\ \hline
Merge     & $\Omega(nlogn)$ & $\Theta(nlogn)$ & $O(nlogn)$ & $O(n)$              \\ \hline
Heap      & $\Omega(nlogn)$ & $\Theta(nlogn)$ & $O(nlogn)$   & $O(n) / O(1)$ \\ \hline
Insertion & $\Omega(n)$     & $\Theta(nlogn)$ & $O(n^2)$   & $O(1)$              \\ \hline
Bucket    & $\Omega(n+k)$   & $\Theta(n+k)$   & $O(n^2)$   & $O(n)$              \\ \hline
\end{tabular}
\end{table}
Here the space complexity of \textit{Quick sort and Heap sort} varies depending
upon the implementation
\subsection{Plots}
\input{../res/plots/all_comp.pgf}
\input{../res/plots/all_swap.pgf}
\input{../res/plots/all_basic.pgf}
\input{../res/plots/all_time.pgf}
\input{../res/plots/all_mem.pgf}
\subsection{Analysis}
As we can see the graphs match the theoretical complexities calculated. Let us
focus on two most important measurements - \textit{Time, Space}.
\subsubsection{Time}
\textit{Insertion sort} with its $O(n^2)$ performs the worst.
The other 4, which all have $O(nlogn)$, perform relatively the same.
The reason why heapsort doesn't perform nearly as good as the other three is
on the hardware level heapsort has \textbf{more instruction that the other three.}
\subsubsection{Space}
Here \textit{Inplace quicksort and insertion sort} are the best as the have
$O(1)$ space complexity. In-place heapsort also has $O(1)$ space complexity
but the ADT used created a new heap from the given array. That is the reason
it has a similar memory use of other $O(n)$ algorithms. But the heapsort
itself \textbf{doesn't} need any auxiliary space.
\section{Optimizing Quick Sort}
\subsection{Problem}
The worst case performance of Quick sort is $O(n^2)$. This occurs when the
\textit{pivot chosen is the greatest or the smallest} causing one of the
partitions to be of size $n-1$.
\subsection{Solution}
This has an easy fix by picking a better pivot. The solution implemented here
is called \texttt{Three median pivot}. Use \textit{Insertion sort} for
partitions \textbf{smaller than a certain threshold(Eg. 10)}
\begin{minted}{python}
    mid = (left + right) // 2
    if arr[mid] < arr[left]:
        arr[left], arr[mid] = arr[mid], arr[left]
    if arr[end] < arr[left]:
        arr[left], arr[end] = arr[end], arr[left]
    if arr[mid] < arr[end]:
        arr[end], arr[mid] = arr[mid], arr[end]
    pivot = arr[end]
    pivot_i = end
\end{minted}
This makes it so that the pivot is never the greatest or the smallest.
\subsection{Testing}
The data given to this particular comparison was sorted in the reverse order
which is the worst case of normal quick sort.
\subsection{Plots}
\subsubsection{Comparisons}
\input{../res/plots/quicksortvsbquicksort_comp.pgf}
\subsubsection{Swaps}
\input{../res/plots/quicksortvsbquicksort_swap.pgf}
\subsubsection{Basic Operations}
\input{../res/plots/quicksortvsbquicksort_basic.pgf}
\subsubsection{Time}
\input{../res/plots/quicksortvsbquicksort_time.pgf}
\subsubsection{Memory}
\input{../res/plots/quicksortvsbquicksort_mem.pgf}
\subsection{Analysis}
As we can see the time taken has \textit{decreased significantly}.
The number of comparison, swaps and basic operations has also shown a
\textit{slight decrease}. The decrease in memory is probably due to
\textit{lesser number of recursion calls}.
\section{Optimizing Merge Sort}
\subsection{Problem}
The main problem with merge sort is not the time complexity as it is one of the
fastest but the \textit{space overhead}. Implemented in its most basic form
it requires $O(n)$ space to create \textit{subarrays} each recursion.
\subsection{Solution}
The solution implemented here is from
\textit{Katajainen, Jyrki; Pasanen, Tomi; Teuhola, Jukka (1996)}.
Instead of creating subarray we pass the original array with index ranges.
The only time subarrays are created is in the \texttt{merge()} routine.
Here \texttt{merge()} creates a copy of the smaller subarray.
\subsection{Plots}
\subsubsection{Comparisons}
\input{../res/plots/mergesortvsbmergesort_comp.pgf}
\subsubsection{Swaps}
\input{../res/plots/mergesortvsbmergesort_swap.pgf}
\subsubsection{Basic Operations}
\input{../res/plots/mergesortvsbmergesort_basic.pgf}
\subsubsection{Time}
\input{../res/plots/mergesortvsbmergesort_time.pgf}
\subsubsection{Memory}
\input{../res/plots/mergesortvsbmergesort_mem.pgf}
\subsection{Analysis}
Since we are practically using only half the memory we see a huge
\textbf{decrease}. The other metrics are very similar. To improve the time
we can use \textbf{timsort} which runs \textit{Insertion sort} upto a point
and switches to merge sort.
\subsubsection{Time}
\input{../res/plots/timsortvsbmergesort_time.pgf}
\subsubsection{Memory}
\input{../res/plots/timsortvsbmergesort_mem.pgf}
\end{document}
